Number of trainable / all parameters: 51156834 / 51156834

SgObjfeatVQDiffusion(
  (network): SgObjfeatTransformerVQDiffusionWrapper(
    (node_embed): Sequential(
      (0): Embedding(23, 512)
      (1): GELU(approximate='none')
      (2): Linear(in_features=512, out_features=512, bias=True)
    )
    (edge_embed): Sequential(
      (0): Embedding(12, 128)
      (1): GELU(approximate='none')
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (time_embed): Sequential(
      (0): Timestep()
      (1): TimestepEmbed(
        (mlp): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
    (objfeat_embed): Embedding(66, 512)
    (objfeat_pool_transformer): ModuleList(
      (0-1): 2 x BasicTransformerBlock(
        (attn): Attention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (ff): FeedForward(
          (mlp): Sequential(
            (0): GEGLU(
              (proj): Linear(in_features=512, out_features=4096, bias=True)
            )
            (1): Dropout(p=0.1, inplace=False)
            (2): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (objfeat_pool_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (objfeat_pool): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=512, out_features=512, bias=True)
    )
    (global_condition_embed): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=512, out_features=512, bias=True)
    )
    (transformer_blocks): ModuleList(
      (0-4): 5 x GraphTransformerBlock(
        (graph_attn): GraphAttention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_e_mul): Linear(in_features=128, out_features=512, bias=False)
          (to_e_add): Linear(in_features=128, out_features=512, bias=False)
          (to_e_out): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (ff_x): FeedForward(
          (mlp): Sequential(
            (0): GEGLU(
              (proj): Linear(in_features=512, out_features=4096, bias=True)
            )
            (1): Dropout(p=0.1, inplace=False)
            (2): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (ff_e): FeedForward(
          (mlp): Sequential(
            (0): GEGLU(
              (proj): Linear(in_features=128, out_features=1024, bias=True)
            )
            (1): Dropout(p=0.1, inplace=False)
            (2): Linear(in_features=512, out_features=128, bias=True)
          )
        )
        (ga_x_norm): AdaLayerNorm(
          (gelu): GELU(approximate='none')
          (linear): Linear(in_features=128, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
        (ff_x_norm): AdaLayerNorm(
          (gelu): GELU(approximate='none')
          (linear): Linear(in_features=128, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
        (ga_e_norm): AdaLayerNorm(
          (gelu): GELU(approximate='none')
          (linear): Linear(in_features=128, out_features=256, bias=True)
          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=False)
        )
        (ff_e_norm): AdaLayerNorm(
          (gelu): GELU(approximate='none')
          (linear): Linear(in_features=128, out_features=256, bias=True)
          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=False)
        )
        (cross_attn): Attention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (ca_norm): AdaLayerNorm(
          (gelu): GELU(approximate='none')
          (linear): Linear(in_features=128, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
      )
    )
    (node_proj_out): Sequential(
      (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (1): Linear(in_features=512, out_features=22, bias=True)
    )
    (edge_proj_out): Sequential(
      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (1): Linear(in_features=128, out_features=11, bias=True)
    )
    (out_objfeat_transformer): ModuleList(
      (0-1): 2 x BasicTransformerBlock(
        (attn): Attention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (ff): FeedForward(
          (mlp): Sequential(
            (0): GEGLU(
              (proj): Linear(in_features=512, out_features=4096, bias=True)
            )
            (1): Dropout(p=0.1, inplace=False)
            (2): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ff_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): Attention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (ca_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (objfeat_out_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (objfeat_out): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=512, out_features=65, bias=True)
    )
  )
)